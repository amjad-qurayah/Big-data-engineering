# Big-data-engineering
This project is a Big Data Engineering endeavor on Azure, involving data ingestion, transformation, and Machine Learning models. It includes creating a Data Lake in Azure, utilizing Azure Data Factory for data processing, and implementing Machine Learning algorithms for text classification. The final output includes visualizations generated through Azure Synapse. 

# Data Ingestion 
we  ingested data from multiple sources into Azure Data Lake. Our data pipeline encompassed the ingestion of datasets from AWS RDS Postgres, Azure Blob Storage, and S3 bucket

# Data Cleaning and Preparation:
The crucial stages of data cleaning and preparation were orchestrated in Databricks, demonstrating our commitment to ensuring high data quality and optimizing datasets for downstream processing. This stage was pivotal in shaping the integrity and usability of the data for subsequent operations.

# Machine Learning Model Training:
Our project involved the training of machine-learning models for tag prediction and questions quality assessment, showcasing our dedication to leveraging advanced analytics and predictive modeling capabilities on the Azure platform. The machine-learning models were rigorously developed and tested, resulting in reliable and accurate insights.

# Workflow Orchestration with Airflow:
A key aspect of our project was integrating Airflow with Azure Data Factory for streamlined workflow orchestration. This integration facilitated the efficient scheduling and monitoring of our data pipelines, further enhancing the reliability and scalability of our data engineering operations.

# Data Visualization and Dashboarding with Azure Synapse and Databricks:
We leveraged the powerful visualization capabilities of Azure Synapse to create visually compelling representations of our results. Furthermore, we developed a comprehensive dashboard using Databricks, providing a user-friendly interface for accessing and interpreting the insights generated from our data engineering efforts.


